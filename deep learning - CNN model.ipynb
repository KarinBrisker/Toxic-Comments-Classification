{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# toxic comments classification\n",
    "## by Karin Brisker\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "import datetime\n",
    "import os\n",
    "import torch.optim as optim\n",
    "import torch.onnx\n",
    "import random\n",
    "import time\n",
    "from torch.nn import DataParallel\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "torch.backends.cudnn.enabled = False\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "from torch.autograd import Variable\n",
    "from tqdm import tqdm\n",
    "import string\n",
    "import itertools\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:2\n"
     ]
    }
   ],
   "source": [
    "lr = 0.01\n",
    "clip = 0.25 # gradient clipping\n",
    "epochs = 20\n",
    "batch_size = 64\n",
    "seed = 1234 # random seed\n",
    "wdecay = 5e-5\n",
    "dropout = 0.5\n",
    "max_norm = 3.0\n",
    "kernel_num = 256\n",
    "kernel_sizes = [3, 4, 5]\n",
    "save_dir = './'\n",
    "\n",
    "# Set the random seed manually for reproducibility.\n",
    "torch.manual_seed(seed)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "device = torch.device(\"cuda:2\")\n",
    "# device = \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load glove embeddings\n",
    "def load_glove():\n",
    "    embeddings_list = []\n",
    "    words = []\n",
    "    embeddings = {}\n",
    "    f = open('./Data/glove.6B.100d.txt', encoding='utf-8')\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        words.append(word)\n",
    "        value = torch.tensor(np.asarray(values[1:], dtype='float32'))\n",
    "        embeddings[word] = value\n",
    "        embeddings_list.append(value)\n",
    "    embeddings[\"<pad>\"] = torch.zeros(100)\n",
    "    embeddings_list.append(torch.zeros(100))\n",
    "    f.close()\n",
    "    vocab_size = len(embeddings.keys())\n",
    "    idx2word = dict(enumerate(embeddings.keys()))\n",
    "    word2idx = dict(zip(idx2word.values(), idx2word.keys()))\n",
    "    return embeddings_list, vocab_size, word2idx, idx2word, embeddings, words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "clean data functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanData(text, remove_stops=False, stemming=False, lemmatization=False):\n",
    "    text = text.lower().split()\n",
    "    text = \" \".join(text)\n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+\\-=]\", \" \", text)\n",
    "\n",
    "    # Replace apostrophes with standard lexicons\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"cannot \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\"!\", \" \", text)\n",
    "    text = re.sub(r\"\\/\", \" \", text)\n",
    "    text = re.sub(r\"\\^\", \" \", text)\n",
    "    text = re.sub(r\"\\+\", \" \", text)\n",
    "    text = re.sub(r\"\\-\", \" \", text)\n",
    "    text = re.sub(r\"\\=\", \" \", text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = re.sub(r\"isn't\", \"is not\", text)\n",
    "    text = re.sub(r\"aren't\", \"are not\", text)\n",
    "    text = re.sub(r\"ain't\", \"am not\", text)\n",
    "    text = re.sub(r\"won't\", \"will not\", text)\n",
    "    text = re.sub(r\"didn't\", \"did not\", text)\n",
    "    text = re.sub(r\"shan't\", \"shall not\", text)\n",
    "    text = re.sub(r\"haven't\", \"have not\", text)\n",
    "    text = re.sub(r\"hadn't\", \"had not\", text)\n",
    "    text = re.sub(r\"hasn't\", \"has not\", text)\n",
    "    text = re.sub(r\"don't\", \"do not\", text)\n",
    "    text = re.sub(r\"wasn't\", \"was not\", text)\n",
    "    text = re.sub(r\"weren't\", \"were not\", text)\n",
    "    text = re.sub(r\"doesn't\", \"does not\", text)\n",
    "    text = re.sub(r\"'s\", \" is\", text)\n",
    "    text = re.sub(r\"'re\", \" are\", text)\n",
    "    text = re.sub(r\"'m\", \" am\", text)\n",
    "    text = re.sub(r\"'d\", \" would\", text)\n",
    "    text = re.sub(r\"'ll\", \" will\", text)\n",
    "    text = re.sub(r\"--th\", \" \", text)\n",
    "    text = re.sub('[()\\\"\\t_\\n.,:=!@#$%^&*-/[\\]?|1234567890—]', ' ', text)\n",
    "\n",
    "    # More cleaning\n",
    "    text = re.sub(r\"alot\", \"a lot\", text)\n",
    "    text = re.sub(r\"what's\", \"\", text)\n",
    "    text = re.sub(r\"What's\", \"\", text)\n",
    "\n",
    "    # Remove urls and emails\n",
    "    text = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', ' ', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'[\\w\\.-]+@[\\w\\.-]+', ' ', text, flags=re.MULTILINE)\n",
    "\n",
    "    # Replace words like sooooooo with so\n",
    "    text = ''.join(''.join(s)[:2] for _, s in itertools.groupby(text))\n",
    "\n",
    "    # Remove punctuation from text\n",
    "    text = ''.join([c for c in text if c not in punctuation])\n",
    "\n",
    "    # Remove all symbols\n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+\\-=]\", \" \", text)\n",
    "    text = re.sub(r'[^A-Za-z\\s]', r' ', text)\n",
    "    text = re.sub(r'\\n', r' ', text)\n",
    "    text = re.sub('[()\\\"\\t_\\n.,:=!@#$%^&*-/[\\]?|1234567890—]', ' ', text)\n",
    "\n",
    "    text = re.sub(r'\\d +', ' ', text)\n",
    "\n",
    "    if remove_stops:\n",
    "        text = \" \".join([w for w in text.split() if w not in stop_words])\n",
    "\n",
    "    if stemming:\n",
    "        st = PorterStemmer()\n",
    "        text = \" \".join([st.stem(w) for w in text.split()])\n",
    "\n",
    "    if lemmatization:\n",
    "        wordnet_lemmatizer = WordNetLemmatizer()\n",
    "        text = \" \".join([wordnet_lemmatizer.lemmatize(w, pos='v') for w in text.split()])\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "special_character_removal = re.compile(r'[^a-z\\d ]', re.IGNORECASE)\n",
    "replace_numbers = re.compile(r'\\d+', re.IGNORECASE)\n",
    "\n",
    "\n",
    "def text_to_wordlist(text):\n",
    "    text = text.lower().split()\n",
    "    text = \" \".join(text)\n",
    "    text = special_character_removal.sub('', text)\n",
    "    text = replace_numbers.sub(' ', text)\n",
    "    return text\n",
    "\n",
    "def new_len(x):\n",
    "    return len(x.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape:  (159571, 8)\n",
      "Test shape:  (153164, 2)\n",
      " ### loading glove ### \n",
      " ### finished loading glove ### \n",
      " ### cleaning data ### \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 5369/137131 [00:00<00:02, 53684.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ### finished cleaning data ### \n",
      " ### text to wordlist ### \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 137131/137131 [00:02<00:00, 54339.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ### finished text to wordlist ### \n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "train_df = pd.read_csv('./Data/train.csv')\n",
    "test_df = pd.read_csv('./Data/test.csv')\n",
    "print('Train shape: ', train_df.shape)\n",
    "print('Test shape: ', test_df.shape)\n",
    "\n",
    "# y values\n",
    "labels_data_train = train_df.iloc[:, 2:].values\n",
    "labels_data_test = test_df.iloc[:, 2:].values\n",
    "labels_count = labels_data_train.sum(axis=0)\n",
    "\n",
    "print(' ### loading glove ### ')\n",
    "\n",
    "embeddings_list, vocab_size, word2idx, idx2word, embeddings, words = load_glove()\n",
    "\n",
    "print(' ### finished loading glove ### ')\n",
    "\n",
    "labels = list(train_df.columns[2:])\n",
    "\n",
    "print(' ### cleaning data ### ')\n",
    "\n",
    "train_df['comment_text'] = train_df['comment_text'].map(lambda x: cleanData(x, True, True, True))\n",
    "train_df = train_df.dropna()\n",
    "train_df = train_df[train_df['comment_text'].map(new_len) > 5]\n",
    "list_sentences_train = train_df[\"comment_text\"].values\n",
    "\n",
    "print(' ### finished cleaning data ### ')\n",
    "\n",
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "y = train_df[list_classes].values\n",
    "list_sentences_test = test_df[\"comment_text\"].values\n",
    "\n",
    "print(' ### text to wordlist ### ')\n",
    "comments = []\n",
    "for text in tqdm(list_sentences_train):\n",
    "    comments.append(text_to_wordlist(text))\n",
    "\n",
    "train_df['comment_text'] = pd.Series(comments)\n",
    "train_df = train_df[train_df['comment_text'].map(type) == str]\n",
    "\n",
    "test_comments = []\n",
    "for text in list_sentences_test:\n",
    "    test_comments.append(text_to_wordlist(text).strip())\n",
    "print(' ### finished text to wordlist ### ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN model for classifing comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNModel(nn.Module):\n",
    "    def __init__(self, n_filters, filter_sizes, output_dim,\n",
    "                 dropout, device, batch_size, embeddings_vectors, vocab_size):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.device = device\n",
    "        D = 100\n",
    "        C = 6\n",
    "        Ci = 1\n",
    "        Co = 256\n",
    "        Ks = [3, 4, 5]\n",
    "        self.embed = nn.Embedding(vocab_size, 100, padding_idx=400000)\n",
    "        self.embed.weight.data.copy_(embeddings_vectors)\n",
    "        self.embed.weight.requires_grad = False\n",
    "        for p in self.embed.parameters():\n",
    "            if p.requires_grad:\n",
    "                p.requires_grad = False\n",
    "\n",
    "        self.convs1 = nn.ModuleList([nn.Conv2d(Ci, Co, (K, D)) for K in Ks])\n",
    "        self.fc_first = nn.Linear(len(filter_sizes) * n_filters, n_filters, bias=True)\n",
    "        self.activation_function = nn.Tanh()\n",
    "        self.device = torch.device('cuda:2')\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc1 = nn.Linear(len(Ks) * Co, C)\n",
    "\n",
    "    def forward(self, comments):\n",
    "        # x = [self.prot_dict[p] for p in protein]\n",
    "        # x = torch.stack(x).to(self.device) # N= 256, w=200, D= 400, C0=256\n",
    "        comments = self.embed(comments)\n",
    "        x = comments.unsqueeze(1)  # (N, Ci, W, D)\n",
    "        x = [F.relu(conv(x)).squeeze(3) for conv in self.convs1]  # [(N, Co, W), ...]*len(Ks)\n",
    "        x = [F.max_pool1d(i, i.size(2)).squeeze(2) for i in x]  # [(N, Co), ...]*len(Ks)\n",
    "        x = torch.cat(x, 1)\n",
    "        x = self.dropout(x)  # (N, len(Ks)*Co) # 256,768 // pooled 256,256\n",
    "        logit = self.fc1(x)# (N, C)\n",
    "        return logit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/1841 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 509,446 trainable parameters\n",
      "epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 1/1841 [00:06<3:13:14,  6.30s/it]\u001b[A\n",
      "  0%|          | 2/1841 [00:10<2:55:39,  5.73s/it]\u001b[A\n",
      "  0%|          | 3/1841 [00:16<2:58:25,  5.82s/it]\u001b[A\n",
      "  0%|          | 4/1841 [00:21<2:48:23,  5.50s/it]\u001b[A\n",
      "  0%|          | 5/1841 [00:24<2:26:29,  4.79s/it]\u001b[A\n",
      "  0%|          | 6/1841 [00:29<2:26:08,  4.78s/it]\u001b[A\n",
      "  0%|          | 7/1841 [00:36<2:50:42,  5.58s/it]\u001b[A\n",
      "  0%|          | 8/1841 [00:41<2:45:28,  5.42s/it]\u001b[A\n",
      "  0%|          | 9/1841 [00:48<2:54:49,  5.73s/it]\u001b[A\n",
      "  1%|          | 10/1841 [00:55<3:11:56,  6.29s/it]\u001b[A\n",
      "  1%|          | 11/1841 [01:01<3:03:49,  6.03s/it]\u001b[A\n",
      "  1%|          | 12/1841 [01:09<3:27:24,  6.80s/it]\u001b[A\n",
      "  1%|          | 13/1841 [01:18<3:43:25,  7.33s/it]\u001b[A\n",
      "  1%|          | 14/1841 [01:25<3:37:17,  7.14s/it]\u001b[A\n",
      "  1%|          | 15/1841 [01:31<3:28:08,  6.84s/it]\u001b[A\n",
      "  1%|          | 16/1841 [01:39<3:38:16,  7.18s/it]\u001b[A\n",
      "  1%|          | 17/1841 [01:48<3:59:46,  7.89s/it]\u001b[A\n",
      "  1%|          | 18/1841 [01:55<3:46:41,  7.46s/it]\u001b[A\n",
      "  1%|          | 19/1841 [02:02<3:41:41,  7.30s/it]\u001b[A\n",
      "  1%|          | 20/1841 [02:10<3:51:28,  7.63s/it]\u001b[A\n",
      "  1%|          | 21/1841 [02:17<3:43:13,  7.36s/it]\u001b[A\n",
      "  1%|          | 22/1841 [02:22<3:23:57,  6.73s/it]\u001b[A\n",
      "  1%|          | 23/1841 [02:28<3:17:58,  6.53s/it]\u001b[A\n",
      "  1%|▏         | 24/1841 [02:35<3:16:37,  6.49s/it]\u001b[A\n",
      "  1%|▏         | 25/1841 [02:41<3:17:44,  6.53s/it]\u001b[A\n",
      "  1%|▏         | 26/1841 [02:50<3:33:52,  7.07s/it]\u001b[A\n",
      "  1%|▏         | 27/1841 [02:59<3:51:31,  7.66s/it]\u001b[A\n",
      "  2%|▏         | 28/1841 [03:06<3:47:33,  7.53s/it]\u001b[A\n",
      "  2%|▏         | 29/1841 [03:12<3:34:28,  7.10s/it]\u001b[A\n",
      "  2%|▏         | 30/1841 [03:17<3:18:14,  6.57s/it]\u001b[A\n",
      "  2%|▏         | 31/1841 [03:25<3:29:57,  6.96s/it]\u001b[A\n",
      "  2%|▏         | 32/1841 [03:32<3:29:46,  6.96s/it]\u001b[A\n",
      "  2%|▏         | 33/1841 [03:38<3:21:52,  6.70s/it]\u001b[A\n",
      "  2%|▏         | 34/1841 [03:48<3:46:45,  7.53s/it]\u001b[A\n",
      "  2%|▏         | 35/1841 [03:54<3:36:30,  7.19s/it]\u001b[A\n",
      "  2%|▏         | 36/1841 [04:01<3:38:29,  7.26s/it]\u001b[A\n",
      "  2%|▏         | 37/1841 [04:08<3:32:01,  7.05s/it]\u001b[A\n",
      "  2%|▏         | 38/1841 [04:14<3:19:03,  6.62s/it]\u001b[A\n",
      "  2%|▏         | 39/1841 [04:21<3:25:05,  6.83s/it]\u001b[A\n",
      "  2%|▏         | 40/1841 [04:28<3:28:08,  6.93s/it]\u001b[A\n",
      "  2%|▏         | 41/1841 [04:33<3:08:06,  6.27s/it]\u001b[A\n",
      "  2%|▏         | 42/1841 [04:42<3:36:22,  7.22s/it]\u001b[A\n",
      "  2%|▏         | 43/1841 [04:48<3:27:19,  6.92s/it]\u001b[A\n",
      "  2%|▏         | 44/1841 [04:56<3:28:20,  6.96s/it]\u001b[A\n",
      "  2%|▏         | 45/1841 [05:06<3:58:59,  7.98s/it]\u001b[A\n",
      "  2%|▏         | 46/1841 [05:13<3:47:20,  7.60s/it]\u001b[A\n",
      "  3%|▎         | 47/1841 [05:21<3:54:43,  7.85s/it]\u001b[A\n",
      "  3%|▎         | 48/1841 [05:28<3:44:54,  7.53s/it]\u001b[A\n",
      "  3%|▎         | 49/1841 [05:36<3:50:17,  7.71s/it]\u001b[A\n",
      "  3%|▎         | 50/1841 [05:43<3:47:40,  7.63s/it]\u001b[A\n",
      "  3%|▎         | 51/1841 [05:51<3:51:31,  7.76s/it]\u001b[A\n",
      "  3%|▎         | 52/1841 [05:58<3:44:10,  7.52s/it]\u001b[A\n",
      "  3%|▎         | 53/1841 [06:04<3:27:18,  6.96s/it]\u001b[A\n",
      "  3%|▎         | 54/1841 [06:11<3:25:07,  6.89s/it]\u001b[A\n",
      "  3%|▎         | 55/1841 [06:18<3:28:10,  6.99s/it]\u001b[A\n",
      "  3%|▎         | 56/1841 [06:24<3:21:00,  6.76s/it]\u001b[A\n",
      "  3%|▎         | 57/1841 [06:30<3:13:16,  6.50s/it]\u001b[A\n",
      "  3%|▎         | 58/1841 [06:37<3:16:53,  6.63s/it]\u001b[A\n",
      "  3%|▎         | 59/1841 [06:44<3:19:32,  6.72s/it]\u001b[A\n",
      "  3%|▎         | 60/1841 [06:55<3:53:27,  7.87s/it]\u001b[A\n",
      "  3%|▎         | 61/1841 [07:05<4:14:01,  8.56s/it]\u001b[A\n",
      "  3%|▎         | 62/1841 [07:11<3:57:26,  8.01s/it]\u001b[A\n",
      "  3%|▎         | 63/1841 [07:16<3:30:47,  7.11s/it]\u001b[A\n",
      "  3%|▎         | 64/1841 [07:23<3:25:08,  6.93s/it]\u001b[A\n",
      "  4%|▎         | 65/1841 [07:31<3:37:18,  7.34s/it]\u001b[A\n",
      "  4%|▎         | 66/1841 [07:38<3:34:46,  7.26s/it]\u001b[A\n",
      "  4%|▎         | 67/1841 [07:46<3:37:47,  7.37s/it]\u001b[A\n",
      "  4%|▎         | 68/1841 [07:56<4:04:08,  8.26s/it]\u001b[A\n",
      "  4%|▎         | 69/1841 [08:05<4:08:54,  8.43s/it]\u001b[A\n",
      "  4%|▍         | 70/1841 [08:12<3:54:00,  7.93s/it]\u001b[A\n",
      "  4%|▍         | 71/1841 [08:18<3:33:59,  7.25s/it]\u001b[A\n",
      "  4%|▍         | 72/1841 [08:23<3:15:56,  6.65s/it]\u001b[A\n",
      "  4%|▍         | 73/1841 [08:31<3:25:34,  6.98s/it]\u001b[A\n",
      "  4%|▍         | 74/1841 [08:36<3:15:48,  6.65s/it]\u001b[A\n",
      "  4%|▍         | 75/1841 [08:47<3:45:59,  7.68s/it]\u001b[A\n",
      "  4%|▍         | 76/1841 [08:55<3:54:41,  7.98s/it]\u001b[A\n",
      "  4%|▍         | 77/1841 [09:06<4:19:04,  8.81s/it]\u001b[A\n",
      "  4%|▍         | 78/1841 [09:12<3:50:18,  7.84s/it]\u001b[A\n",
      "  4%|▍         | 79/1841 [09:19<3:51:32,  7.88s/it]\u001b[A\n",
      "  4%|▍         | 80/1841 [09:28<4:00:41,  8.20s/it]\u001b[A\n",
      "  4%|▍         | 81/1841 [09:35<3:42:55,  7.60s/it]\u001b[A\n",
      "  4%|▍         | 82/1841 [09:41<3:33:16,  7.27s/it]\u001b[A\n",
      "  5%|▍         | 83/1841 [09:48<3:32:40,  7.26s/it]\u001b[A\n",
      "  5%|▍         | 84/1841 [09:56<3:33:26,  7.29s/it]\u001b[A\n",
      "  5%|▍         | 85/1841 [10:03<3:35:45,  7.37s/it]\u001b[A\n",
      "  5%|▍         | 86/1841 [10:14<4:03:04,  8.31s/it]\u001b[A\n",
      "  5%|▍         | 87/1841 [10:20<3:45:48,  7.72s/it]\u001b[A\n",
      "  5%|▍         | 88/1841 [10:27<3:40:34,  7.55s/it]\u001b[A\n",
      "  5%|▍         | 89/1841 [10:36<3:48:44,  7.83s/it]\u001b[A\n",
      "  5%|▍         | 90/1841 [10:44<3:47:35,  7.80s/it]\u001b[A\n",
      "  5%|▍         | 91/1841 [10:50<3:39:37,  7.53s/it]\u001b[A\n",
      "  5%|▍         | 92/1841 [10:58<3:42:02,  7.62s/it]\u001b[A\n",
      "  5%|▌         | 93/1841 [11:05<3:31:23,  7.26s/it]\u001b[A\n",
      "  5%|▌         | 94/1841 [11:15<3:54:28,  8.05s/it]\u001b[A\n",
      "  5%|▌         | 95/1841 [11:20<3:32:46,  7.31s/it]\u001b[A\n",
      "  5%|▌         | 96/1841 [11:28<3:35:06,  7.40s/it]\u001b[A\n",
      "  5%|▌         | 97/1841 [11:36<3:45:10,  7.75s/it]\u001b[A\n",
      "  5%|▌         | 98/1841 [11:46<3:58:07,  8.20s/it]\u001b[A\n",
      "  5%|▌         | 99/1841 [11:52<3:46:36,  7.80s/it]\u001b[A\n",
      "  5%|▌         | 100/1841 [11:59<3:34:14,  7.38s/it]\u001b[A\n",
      "  5%|▌         | 101/1841 [12:05<3:26:12,  7.11s/it]\u001b[A\n",
      "  6%|▌         | 102/1841 [12:11<3:18:00,  6.83s/it]\u001b[A\n",
      "  6%|▌         | 103/1841 [12:19<3:22:23,  6.99s/it]\u001b[A\n",
      "  6%|▌         | 104/1841 [12:25<3:12:44,  6.66s/it]\u001b[A\n",
      "  6%|▌         | 105/1841 [12:32<3:18:35,  6.86s/it]\u001b[A\n",
      "  6%|▌         | 106/1841 [12:37<2:59:28,  6.21s/it]\u001b[A\n",
      "  6%|▌         | 107/1841 [12:45<3:13:40,  6.70s/it]\u001b[A\n",
      "  6%|▌         | 108/1841 [12:54<3:35:49,  7.47s/it]\u001b[A\n",
      "  6%|▌         | 109/1841 [13:02<3:40:01,  7.62s/it]\u001b[A\n",
      "  6%|▌         | 110/1841 [13:10<3:43:22,  7.74s/it]\u001b[A\n",
      "  6%|▌         | 111/1841 [13:17<3:37:41,  7.55s/it]\u001b[A\n",
      "  6%|▌         | 112/1841 [13:26<3:46:16,  7.85s/it]\u001b[A\n",
      "  6%|▌         | 113/1841 [13:34<3:51:42,  8.05s/it]\u001b[A\n",
      "  6%|▌         | 114/1841 [13:40<3:32:48,  7.39s/it]\u001b[A\n",
      "  6%|▌         | 115/1841 [13:46<3:24:42,  7.12s/it]\u001b[A\n",
      "  6%|▋         | 116/1841 [13:55<3:41:01,  7.69s/it]\u001b[A\n",
      "  6%|▋         | 117/1841 [14:05<3:56:35,  8.23s/it]\u001b[A\n",
      "  6%|▋         | 118/1841 [14:15<4:09:41,  8.70s/it]\u001b[A\n",
      "  6%|▋         | 119/1841 [14:21<3:53:28,  8.13s/it]\u001b[A\n",
      "  7%|▋         | 120/1841 [14:28<3:43:31,  7.79s/it]\u001b[A\n",
      "  7%|▋         | 121/1841 [14:35<3:29:43,  7.32s/it]\u001b[A\n",
      "  7%|▋         | 122/1841 [14:41<3:24:12,  7.13s/it]\u001b[A\n",
      "  7%|▋         | 123/1841 [14:48<3:18:55,  6.95s/it]\u001b[A\n",
      "  7%|▋         | 124/1841 [14:55<3:20:22,  7.00s/it]\u001b[A\n",
      "  7%|▋         | 125/1841 [15:01<3:13:03,  6.75s/it]\u001b[A\n",
      "  7%|▋         | 126/1841 [15:08<3:15:15,  6.83s/it]\u001b[A\n",
      "  7%|▋         | 127/1841 [15:16<3:19:03,  6.97s/it]\u001b[A\n",
      "  7%|▋         | 128/1841 [15:24<3:30:55,  7.39s/it]\u001b[A\n",
      "  7%|▋         | 129/1841 [15:30<3:18:09,  6.94s/it]\u001b[A\n",
      "  7%|▋         | 130/1841 [15:42<4:07:06,  8.67s/it]\u001b[A\n",
      "  7%|▋         | 131/1841 [15:49<3:46:29,  7.95s/it]\u001b[A\n",
      "  7%|▋         | 132/1841 [15:53<3:13:57,  6.81s/it]\u001b[A\n",
      "  7%|▋         | 133/1841 [16:01<3:24:46,  7.19s/it]\u001b[A\n",
      "  7%|▋         | 134/1841 [16:07<3:10:44,  6.70s/it]\u001b[A\n",
      "  7%|▋         | 135/1841 [16:17<3:42:08,  7.81s/it]\u001b[A\n",
      "  7%|▋         | 136/1841 [16:21<3:12:01,  6.76s/it]\u001b[A\n",
      "  7%|▋         | 137/1841 [16:27<3:05:59,  6.55s/it]\u001b[A\n",
      "  7%|▋         | 138/1841 [16:34<3:09:10,  6.66s/it]\u001b[A\n",
      "  8%|▊         | 139/1841 [16:41<3:07:34,  6.61s/it]\u001b[A\n",
      "  8%|▊         | 140/1841 [16:46<2:57:41,  6.27s/it]\u001b[A\n",
      "  8%|▊         | 141/1841 [16:53<2:58:02,  6.28s/it]\u001b[A\n",
      "  8%|▊         | 142/1841 [16:58<2:48:33,  5.95s/it]\u001b[A\n",
      "  8%|▊         | 143/1841 [17:01<2:28:46,  5.26s/it]\u001b[A\n",
      "  8%|▊         | 144/1841 [17:06<2:23:45,  5.08s/it]\u001b[A\n",
      "  8%|▊         | 145/1841 [17:12<2:29:32,  5.29s/it]\u001b[A\n",
      "  8%|▊         | 146/1841 [17:16<2:22:25,  5.04s/it]\u001b[A\n",
      "  8%|▊         | 147/1841 [17:21<2:19:27,  4.94s/it]\u001b[A\n",
      "  8%|▊         | 148/1841 [17:28<2:36:23,  5.54s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 149/1841 [17:32<2:28:07,  5.25s/it]\u001b[A\n",
      "  8%|▊         | 150/1841 [17:38<2:30:17,  5.33s/it]\u001b[A\n",
      "  8%|▊         | 151/1841 [17:43<2:27:10,  5.23s/it]\u001b[A\n",
      "  8%|▊         | 152/1841 [17:48<2:24:37,  5.14s/it]\u001b[A\n",
      "  8%|▊         | 153/1841 [17:54<2:30:41,  5.36s/it]\u001b[A\n",
      "  8%|▊         | 154/1841 [17:58<2:24:22,  5.13s/it]\u001b[A\n",
      "  8%|▊         | 155/1841 [18:03<2:23:23,  5.10s/it]\u001b[A\n",
      "  8%|▊         | 156/1841 [18:09<2:27:54,  5.27s/it]\u001b[A\n",
      "  9%|▊         | 157/1841 [18:14<2:27:44,  5.26s/it]\u001b[A\n",
      "  9%|▊         | 158/1841 [18:20<2:31:02,  5.38s/it]\u001b[A\n",
      "  9%|▊         | 159/1841 [18:25<2:26:13,  5.22s/it]\u001b[A\n",
      "  9%|▊         | 160/1841 [18:31<2:38:22,  5.65s/it]\u001b[A\n",
      "  9%|▊         | 161/1841 [18:35<2:23:08,  5.11s/it]\u001b[A\n",
      "  9%|▉         | 162/1841 [18:41<2:27:50,  5.28s/it]\u001b[A\n",
      "  9%|▉         | 163/1841 [18:45<2:19:56,  5.00s/it]\u001b[A\n",
      "  9%|▉         | 164/1841 [18:53<2:37:49,  5.65s/it]\u001b[A\n",
      "  9%|▉         | 165/1841 [18:58<2:33:14,  5.49s/it]\u001b[A\n",
      "  9%|▉         | 166/1841 [19:03<2:32:40,  5.47s/it]\u001b[A\n",
      "  9%|▉         | 167/1841 [19:07<2:19:20,  4.99s/it]\u001b[A\n",
      "  9%|▉         | 168/1841 [19:12<2:17:16,  4.92s/it]\u001b[A\n",
      "  9%|▉         | 169/1841 [19:16<2:15:32,  4.86s/it]\u001b[A\n",
      "  9%|▉         | 170/1841 [19:22<2:20:42,  5.05s/it]\u001b[A\n",
      "  9%|▉         | 171/1841 [19:26<2:09:05,  4.64s/it]\u001b[A\n",
      "  9%|▉         | 172/1841 [19:30<2:07:55,  4.60s/it]\u001b[A\n",
      "  9%|▉         | 173/1841 [19:36<2:16:56,  4.93s/it]\u001b[A\n",
      "  9%|▉         | 174/1841 [19:40<2:07:36,  4.59s/it]\u001b[A\n",
      " 10%|▉         | 175/1841 [19:47<2:33:58,  5.55s/it]\u001b[A\n",
      " 10%|▉         | 176/1841 [19:52<2:26:49,  5.29s/it]\u001b[A"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-9ca67bbb94d0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'epoch: '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0mepoch_start_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'-'\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m89\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'epoch {:3d} | time: {:5.2f}s'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mepoch_start_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-9ca67bbb94d0>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(train_data, model)\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mmy_generator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0miter\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatches_range\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mcomments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmy_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomments\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mcount\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-9ca67bbb94d0>\u001b[0m in \u001b[0;36mget_batch\u001b[0;34m(train_df)\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mbatch_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_comments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m             \u001b[0mwords_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mword2idx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch_comments\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m             \u001b[0mbatch_idx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mprotein_tensors_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword2idx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'<pad>'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-9ca67bbb94d0>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mbatch_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_comments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m             \u001b[0mwords_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mword2idx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch_comments\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m             \u001b[0mbatch_idx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mprotein_tensors_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword2idx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'<pad>'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def get_batch(train_df):\n",
    "    num_samples = train_df.shape[0]\n",
    "    remainder = num_samples % batch_size\n",
    "    for j in range(0, num_samples - remainder, batch_size):\n",
    "        batch = train_df[j: j + batch_size]\n",
    "        batch_comments = batch[\"comment_text\"].values\n",
    "        batch_labels = [torch.tensor(x) for x in batch[list_classes].values]\n",
    "        batch_idx = []\n",
    "        for i in range(len(batch_comments)):\n",
    "            words_data = [word2idx[c] for c in batch_comments[i].split() if c in words]\n",
    "            batch_idx.append(torch.LongTensor(words_data))\n",
    "        protein_tensors_list = torch.nn.utils.rnn.pad_sequence(batch_idx, True, word2idx['<pad>'])\n",
    "        yield protein_tensors_list, torch.stack(batch_labels)\n",
    "\n",
    "\n",
    "def train(train_data, model):\n",
    "    # Turn on training mode which enables dropout.\n",
    "    model.train()\n",
    "    count = 0\n",
    "    total_loss, correct = 0., 0\n",
    "    # shuffle data\n",
    "    train_data = train_data.sample(frac=1).reset_index(drop=True)\n",
    "    start_time = time.time()\n",
    "    batches_range = int((len(train_data) - 1) / batch_size)\n",
    "    my_generator = get_batch(train_data)\n",
    "    for iter in tqdm(range(batches_range)):\n",
    "        comments, targets = next(my_generator)\n",
    "        output = model(comments.to(device))\n",
    "        count += int(batch_size)\n",
    "        loss = criterion(output.float(), Variable(targets).float().to(device))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "        torch.nn.utils.clip_grad_norm_(params, clip)\n",
    "        optimizer.step()\n",
    "        total_loss += loss.detach()\n",
    "    print('\\n')\n",
    "    print('-' * 89)\n",
    "    print(\n",
    "        '| time: {:5.2f}s | train loss {:5.8f} | lr {:2.5f}'.format((time.time() - start_time),\n",
    "                                                                                             total_loss * 1.0 / count,\n",
    "                                                                                             optimizer.param_groups[0][\n",
    "                                                                                                 'lr']))\n",
    "    print('-' * 89)\n",
    "    return\n",
    "\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    embeddings_vectors = torch.stack(embeddings_list)\n",
    "    model = CNNModel(kernel_num, kernel_sizes, 2, 0.5, device, batch_size, embeddings_vectors,\n",
    "                      len(word2idx)).to(device)\n",
    "    model = DataParallel(model, device_ids=[2, 1, 0, 3], output_device=2)  # run on all 4 gpus\n",
    "    print(f'The model has {count_parameters(model):,} trainable parameters')\n",
    "    criterion = nn.BCEWithLogitsLoss().to(device)\n",
    "    params_model = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    params = list(params_model) + list(criterion.parameters())\n",
    "    optimizer = optim.Adam(params, lr=lr, weight_decay=wdecay)\n",
    "\n",
    "    # Loop over epochs.\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        print('epoch: ' + str(epoch))\n",
    "        epoch_start_time = time.time()\n",
    "        train(train_df, model)\n",
    "        print('-' * 89)\n",
    "        print('epoch {:3d} | time: {:5.2f}s'.format(epoch, (time.time() - epoch_start_time)))\n",
    "        print('-' * 89)\n",
    "        if not os.path.isdir(save_dir):\n",
    "            os.makedirs(save_dir)\n",
    "\n",
    "        with open(os.path.join(save_dir, 'epoch_' + str(epoch) + '.pt'), 'wb') as f:\n",
    "            torch.save(model.module.state_dict(), f)\n",
    "\n",
    "    print('=' * 89)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
